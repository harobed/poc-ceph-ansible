# Tests incident

## Coupure d'un pod

Avant la coupure:

```
root@ceph-test-1:/home/vagrant# ceph health detail
HEALTH_OK

root@ceph-test-1:/home/vagrant# ceph osd tree
ID WEIGHT  TYPE NAME            UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 1.44955 root default
-2 0.48318     host ceph-test-2
 0 0.48318         osd.0             up  1.00000          1.00000
-3 0.48318     host ceph-test-3
 2 0.48318         osd.2             up  1.00000          1.00000
-4 0.48318     host ceph-test-1
 1 0.48318         osd.1             up  1.00000          1.00000

root@ceph-test-1:/home/vagrant# ceph status
    cluster 7ecb6ebd-2e7a-44c3-bf0d-ff8d193e03ac
     health HEALTH_OK
     monmap e1: 3 mons at {ceph-test-1=172.28.128.3:6789/0,ceph-test-2=172.28.128.4:6789/0,ceph-test-3=172.28.128.6:6789/0}
            election epoch 8, quorum 0,1,2 ceph-test-1,ceph-test-2,ceph-test-3
     osdmap e20: 3 osds: 3 up, 3 in
            flags sortbitwise,require_jewel_osds
      pgmap v59: 64 pgs, 1 pools, 21772 kB data, 33 objects
            126 MB used, 1484 GB / 1484 GB avail
                  64 active+clean
```

```
$ vagrant ssh ceph-test-2
root@ceph-test-2:/home/vagrant# systemctl --no-pager -t service | grep "ceph"
ceph-mon@ceph-test-2.service       loaded active running Ceph cluster monitor daemon
ceph-osd@0.service                 loaded active running Ceph object storage daemon

root@ceph-test-2:/home/vagrant# systemctl stop ceph-osd@0.service

root@ceph-test-2:/home/vagrant# systemctl --no-pager -t service | grep "ceph"
ceph-mon@ceph-test-2.service       loaded active running Ceph cluster monitor daemon
```


```
root@ceph-test-1:/home/vagrant# ceph status
    cluster 7ecb6ebd-2e7a-44c3-bf0d-ff8d193e03ac
     health HEALTH_WARN
            clock skew detected on mon.ceph-test-3
            41 pgs degraded
            22 pgs stuck unclean
            41 pgs undersized
            recovery 16/66 objects degraded (24.242%)
            too few PGs per OSD (29 < min 30)
            1/3 in osds are down
            Monitor clock skew detected
     monmap e1: 3 mons at {ceph-test-1=172.28.128.3:6789/0,ceph-test-2=172.28.128.4:6789/0,ceph-test-3=172.28.128.6:6789/0}
            election epoch 8, quorum 0,1,2 ceph-test-1,ceph-test-2,ceph-test-3
     osdmap e27: 3 osds: 2 up, 3 in; 41 remapped pgs
            flags sortbitwise,require_jewel_osds
      pgmap v83: 64 pgs, 1 pools, 21772 kB data, 33 objects
            128 MB used, 1484 GB / 1484 GB avail
            16/66 objects degraded (24.242%)
                  41 active+undersized+degraded
                  23 active+clean
```

```
root@ceph-test-2:/home/vagrant# systemctl stop ceph-mon@ceph-test-2.service
```

```
root@ceph-test-1:/home/vagrant# ceph status
    cluster 7ecb6ebd-2e7a-44c3-bf0d-ff8d193e03ac
     health HEALTH_WARN
            1 mons down, quorum 0,2 ceph-test-1,ceph-test-3
     monmap e1: 3 mons at {ceph-test-1=172.28.128.3:6789/0,ceph-test-2=172.28.128.4:6789/0,ceph-test-3=172.28.128.6:6789/0}
            election epoch 10, quorum 0,2 ceph-test-1,ceph-test-3
     osdmap e30: 3 osds: 2 up, 2 in
            flags sortbitwise,require_jewel_osds
      pgmap v100: 64 pgs, 1 pools, 21772 kB data, 33 objects
            98128 kB used, 989 GB / 989 GB avail
                  64 active+clean
```

```
$ vagrant halt ceph-test-2
```

```
root@ceph-test-1:/home/vagrant# ceph status
    cluster 7ecb6ebd-2e7a-44c3-bf0d-ff8d193e03ac
     health HEALTH_WARN
            1 mons down, quorum 0,2 ceph-test-1,ceph-test-3
     monmap e1: 3 mons at {ceph-test-1=172.28.128.3:6789/0,ceph-test-2=172.28.128.4:6789/0,ceph-test-3=172.28.128.6:6789/0}
            election epoch 10, quorum 0,2 ceph-test-1,ceph-test-3
     osdmap e30: 3 osds: 2 up, 2 in
            flags sortbitwise,require_jewel_osds
      pgmap v100: 64 pgs, 1 pools, 21772 kB data, 33 objects
            98128 kB used, 989 GB / 989 GB avail
                  64 active+clean
```

```
$  vagrant up ceph-test-2
root@ceph-test-2:/home/vagrant# systemctl --no-pager -t service | grep "ceph"
ceph-mon@ceph-test-2.service       loaded active running Ceph cluster monitor daemon
ceph-osd@0.service                 loaded active running Ceph object storage daemon
ceph.service                       loaded active exited  LSB: Start Ceph distributed file system daemons at boot time
```

```
root@ceph-test-1:/home/vagrant# ceph status
    cluster 7ecb6ebd-2e7a-44c3-bf0d-ff8d193e03ac
     health HEALTH_OK
     monmap e1: 3 mons at {ceph-test-1=172.28.128.3:6789/0,ceph-test-2=172.28.128.4:6789/0,ceph-test-3=172.28.128.6:6789/0}
            election epoch 12, quorum 0,1,2 ceph-test-1,ceph-test-2,ceph-test-3
     osdmap e33: 3 osds: 3 up, 3 in
            flags sortbitwise,require_jewel_osds
      pgmap v112: 64 pgs, 1 pools, 21772 kB data, 33 objects
            128 MB used, 1484 GB / 1484 GB avail
                  64 active+clean
```

```
$ vagrant halt ceph-test-2
```

```
root@ceph-test-1:/home/vagrant# ceph status
    cluster 7ecb6ebd-2e7a-44c3-bf0d-ff8d193e03ac
     health HEALTH_WARN
            41 pgs degraded
            22 pgs stuck unclean
            41 pgs undersized
            recovery 16/66 objects degraded (24.242%)
            too few PGs per OSD (29 < min 30)
            1/3 in osds are down
            1 mons down, quorum 0,2 ceph-test-1,ceph-test-3
     monmap e1: 3 mons at {ceph-test-1=172.28.128.3:6789/0,ceph-test-2=172.28.128.4:6789/0,ceph-test-3=172.28.128.6:6789/0}
            election epoch 14, quorum 0,2 ceph-test-1,ceph-test-3
     osdmap e35: 3 osds: 2 up, 3 in; 41 remapped pgs
            flags sortbitwise,require_jewel_osds
      pgmap v118: 64 pgs, 1 pools, 21772 kB data, 33 objects
            130 MB used, 1484 GB / 1484 GB avail
            16/66 objects degraded (24.242%)
                  41 active+undersized+degraded
                  23 active+clean
```

Next IÂ copy some data:

```
$ vagrant ssh ceph-client-1
$ sudo su
# rbd map rbd/image1 --id admin
# mkdir -p /mnt/image1
# mount /dev/rbd0 /mnt/image1
# cd /mnt/image1/
# curl https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.9.tar.xz -o linux-4.9.tar.xz
# tar xf linux-4.9.tar.xz
# du ./ -h -s
845M	./
```

```
root@ceph-test-1:/home/vagrant# ceph osd tree
ID WEIGHT  TYPE NAME            UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 1.44955 root default
-2 0.48318     host ceph-test-2
 0 0.48318         osd.0           down        0          1.00000
-3 0.48318     host ceph-test-3
 2 0.48318         osd.2             up  1.00000          1.00000
-4 0.48318     host ceph-test-1
 1 0.48318         osd.1             up  1.00000          1.00000
```

```
$ root@ceph-test-1:/home/vagrant# ceph -w
    cluster 7ecb6ebd-2e7a-44c3-bf0d-ff8d193e03ac
     health HEALTH_WARN
            clock skew detected on mon.ceph-test-3
            1 mons down, quorum 0,2 ceph-test-1,ceph-test-3
            Monitor clock skew detected
     monmap e1: 3 mons at {ceph-test-1=172.28.128.3:6789/0,ceph-test-2=172.28.128.4:6789/0,ceph-test-3=172.28.128.6:6789/0}
            election epoch 14, quorum 0,2 ceph-test-1,ceph-test-3
     osdmap e38: 3 osds: 2 up, 2 in
            flags sortbitwise,require_jewel_osds
      pgmap v300: 64 pgs, 1 pools, 154 MB data, 60 objects
            365 MB used, 989 GB / 989 GB avail
                  64 active+clean

2016-12-20 13:53:25.386370 mon.0 [WRN] mon.2 172.28.128.6:6789/0 clock skew 0.100698s > max 0.05s
2016-12-20 13:54:55.387536 mon.0 [WRN] mon.2 172.28.128.6:6789/0 clock skew 0.11446s > max 0.05s
```

```
$ vagrant up ceph-test-2
```

Now I create fourth OSD and Mon.

Now I shutdown ceph-test-1 and ceph-test-3

```
$ vagrant halt ceph-test-1
$ vagrant halt ceph-test-3
```
